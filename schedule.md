# NLP经典论文阅读计划

在这里小小立个flag， 每天抽出两个小时读一篇NLP经典论文（或综述），一周写一份阅读笔记。

## Week 1

出于我自己的实际情况需要，前三周要跟transformer->bert->gpt杠上了。。。之后再补更基础的一些文章。

- [ ] [Pre-trained Models for Natural Language Processing: A Survey](https://www.researchgate.net/profile/Tianxiang-Sun/publication/346260400_Pre-trained_models_for_natural_language_processing_A_survey/links/6170e228766c4a211c030b64/Pre-trained-models-for-natural-language-processing-A-survey.pdf) (综述)

- [ ] [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (精读)

- [ ] [Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)

## Week 2

- [ ] [Pre-trained models: Past, present and future](https://keg.cs.tsinghua.edu.cn/jietang/publications/AIOPEN21-Han-et-al-Pre-Trained%20Models-%20Past,%20Present%20and%20Future.pdf) (综述)
